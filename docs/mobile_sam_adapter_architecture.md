# Mobile SAM Adapter 架构图

## 整体架构示意图

```
┌─────────────────────────────────────────────────────────────────────────────────┐
│                           Mobile_sam_adapter                                      │
│                                                                                   │
│  ┌─────────────────────────────────────────────────────────────────────────────┐ │
│  │                         输入图像 (B, 3, H, W)                                 │ │
│  └─────────────────────────────────────────────────────────────────────────────┘ │
│                                      │                                            │
│                                      ▼                                            │
│  ╔═════════════════════════════════════════════════════════════════════════════╗ │
│  ║                    IMAGE ENCODER (TinyViT)                                   ║ │
│  ╠═════════════════════════════════════════════════════════════════════════════╣ │
│  ║                                                                              ║ │
│  ║  ┌─────────────────────────────────────────────────────────────────────┐    ║ │
│  ║  │  PatchEmbed: Conv2d_BN (3→32→64, stride=4)                          │    ║ │
│  ║  │  输出: (B, 64, H/4, W/4)                                             │    ║ │
│  ║  └─────────────────────────────────────────────────────────────────────┘    ║ │
│  ║                              │                                               ║ │
│  ║          ┌───────────────────┼───────────────────┐                          ║ │
│  ║          │                   │                   │                          ║ │
│  ║          ▼                   ▼                   ▼                          ║ │
│  ║  ┌─────────────┐    ┌─────────────────┐   ┌──────────────────┐             ║ │
│  ║  │ FFT滤波器    │    │  MobileNetV3    │   │  主干网络        │             ║ │
│  ║  │ (高通滤波)   │    │    分支          │   │                  │             ║ │
│  ║  │             │    │                  │   │                  │             ║ │
│  ║  └──────┬──────┘    └────────┬─────────┘   │                  │             ║ │
│  ║         │                    │             │                  │             ║ │
│  ║         ▼                    ▼             │                  │             ║ │
│  ║  ┌─────────────────────────────────────┐   │                  │             ║ │
│  ║  │       PromptGenerator               │   │                  │             ║ │
│  ║  │  handcrafted + embedding features   │◄──┼──────────────────┤             ║ │
│  ║  └─────────────────────────────────────┘   │                  │             ║ │
│  ║                              │             │                  │             ║ │
│  ║                              ▼             ▼                  │             ║ │
│  ║  ┌─────────────────────────────────────────────────────────┐ │             ║ │
│  ║  │                  STAGE 1 (ConvLayer)                    │ │             ║ │
│  ║  │  ┌─────────┐    ┌─────────┐    ┌────────────────┐      │ │             ║ │
│  ║  │  │ MBConv  │───►│ MBConv  │───►│ PatchMerging   │      │ │             ║ │
│  ║  │  │  ×2     │    │         │    │ (64→128)       │      │ │             ║ │
│  ║  │  └─────────┘    └─────────┘    └────────────────┘      │ │             ║ │
│  ║  │  输出: (B, 128, H/8, W/8)                               │ │             ║ │
│  ║  └─────────────────────────────────────────────────────────┘ │             ║ │
│  ║                              │                               │             ║ │
│  ║                              ▼                               │             ║ │
│  ║  ┌─────────────────────────────────────────────────────────┐ │             ║ │
│  ║  │                  STAGE 2 (BasicLayer)                   │ │             ║ │
│  ║  │  ┌──────────────┐  ┌──────────────┐  ┌──────────────┐  │ │             ║ │
│  ║  │  │ TinyViTBlock │─►│ TinyViTBlock │─►│ PatchMerging │  │ │             ║ │
│  ║  │  │    ×2        │  │              │  │ (128→160)    │  │ │             ║ │
│  ║  │  │ ┌──────────┐ │  │              │  │              │  │ │             ║ │
│  ║  │  │ │Window    │ │  │              │  │              │  │ │             ║ │
│  ║  │  │ │Attention │ │  │              │  │              │  │ │             ║ │
│  ║  │  │ └──────────┘ │  │              │  │              │  │ │             ║ │
│  ║  │  │ ┌──────────┐ │  │              │  │              │  │ │             ║ │
│  ║  │  │ │LocalConv │ │  │              │  │              │  │ │             ║ │
│  ║  │  │ └──────────┘ │  │              │  │              │  │ │             ║ │
│  ║  │  │ ┌──────────┐ │  │              │  │              │  │ │             ║ │
│  ║  │  │ │   MLP    │ │  │              │  │              │  │ │             ║ │
│  ║  │  │ └──────────┘ │  │              │  │              │  │ │             ║ │
│  ║  │  └──────────────┘  └──────────────┘  └──────────────┘  │ │             ║ │
│  ║  │  输出: (B, 160, H/16, W/16)                             │ │             ║ │
│  ║  └─────────────────────────────────────────────────────────┘ │             ║ │
│  ║                              │                               │             ║ │
│  ║                              ▼                               │             ║ │
│  ║  ┌─────────────────────────────────────────────────────────┐ │             ║ │
│  ║  │                  STAGE 3 (BasicLayer)                   │ │             ║ │
│  ║  │  TinyViTBlock × 6 + PatchMerging (160→320)             │ │             ║ │
│  ║  │  输出: (B, 320, H/16, W/16)                              │ │             ║ │
│  ║  └─────────────────────────────────────────────────────────┘ │             ║ │
│  ║                              │                               │             ║ │
│  ║                              ▼                               │             ║ │
│  ║  ┌─────────────────────────────────────────────────────────┐ │             ║ │
│  ║  │                  STAGE 4 (BasicLayer)                   │ │             ║ │
│  ║  │  TinyViTBlock × 2 (无下采样)                            │ │             ║ │
│  ║  │  输出: (B, 320, H/16, W/16)                              │ │             ║ │
│  ║  └─────────────────────────────────────────────────────────┘ │             ║ │
│  ║                              │                               │             ║ │
│  ║                              ▼                               │             ║ │
│  ║  ┌─────────────────────────────────────────────────────────┐ │             ║ │
│  ║  │                       NECK                               │ │             ║ │
│  ║  │  Conv2d(320→256) + LN + Conv2d(256→256,3×3) + LN       │ │             ║ │
│  ║  │  输出: (B, 256, H/16, W/16)                              │ │             ║ │
│  ║  └─────────────────────────────────────────────────────────┘ │             ║ │
│  ╚═════════════════════════════════════════════════════════════════════════════╝ │
│                                      │                                            │
│                                      │ image_embeddings                           │
│                                      ▼                                            │
│  ╔═════════════════════════════════════════════════════════════════════════════╗ │
│  ║                         MASK DECODER                                         ║ │
│  ╠═════════════════════════════════════════════════════════════════════════════╣ │
│  ║                                                                              ║ │
│  ║   ┌────────────────┐    ┌────────────────┐    ┌────────────────────┐        ║ │
│  ║   │  iou_token     │    │  mask_tokens   │    │ PositionEmbedding  │        ║ │
│  ║   │  Embedding(1)  │    │  Embedding(4)  │    │    (Random PE)     │        ║ │
│  ║   └───────┬────────┘    └───────┬────────┘    └─────────┬──────────┘        ║ │
│  ║           │                     │                       │                    ║ │
│  ║           └──────────┬──────────┘                       │                    ║ │
│  ║                      │ tokens                           │ image_pe           ║ │
│  ║                      ▼                                  ▼                    ║ │
│  ║   ┌─────────────────────────────────────────────────────────────────────┐   ║ │
│  ║   │              TwoWayTransformer (depth=2)                             │   ║ │
│  ║   │  ┌─────────────────────────────────────────────────────────────┐    │   ║ │
│  ║   │  │  TwoWayAttentionBlock ×2                                     │    │   ║ │
│  ║   │  │  ┌──────────────┐  ┌──────────────────┐  ┌──────────────┐   │    │   ║ │
│  ║   │  │  │  Self-Attn   │─►│ Cross-Attn       │─►│    MLP       │   │    │   ║ │
│  ║   │  │  │  (queries)   │  │ (token→image)    │  │              │   │    │   ║ │
│  ║   │  │  └──────────────┘  └──────────────────┘  └──────┬───────┘   │    │   ║ │
│  ║   │  │                                                 │           │    │   ║ │
│  ║   │  │                           ┌─────────────────────┘           │    │   ║ │
│  ║   │  │                           ▼                                 │    │   ║ │
│  ║   │  │                    ┌──────────────────┐                     │    │   ║ │
│  ║   │  │                    │ Cross-Attn       │                     │    │   ║ │
│  ║   │  │                    │ (image→token)    │                     │    │   ║ │
│  ║   │  │                    └──────────────────┘                     │    │   ║ │
│  ║   │  └─────────────────────────────────────────────────────────────┘    │   ║ │
│  ║   │                                                                      │   ║ │
│  ║   │  ┌─────────────────────────────────────────────────────────────┐    │   ║ │
│  ║   │  │  Final Attention (token→image) + LayerNorm                   │    │   ║ │
│  ║   │  └─────────────────────────────────────────────────────────────┘    │   ║ │
│  ║   └─────────────────────────────────────────────────────────────────────┘   ║ │
│  ║                      │                      │                                ║ │
│  ║           queries    │           src        │                                ║ │
│  ║                      ▼                      ▼                                ║ │
│  ║   ┌──────────────────────┐    ┌──────────────────────────────────────────┐  ║ │
│  ║   │  iou_prediction_head │    │          output_upscaling                │  ║ │
│  ║   │  MLP(256→256→4)      │    │  ConvT(256→64,2×) + ConvT(64→32,2×)     │  ║ │
│  ║   └──────────────────────┘    └──────────────────────────────────────────┘  ║ │
│  ║           │                                  │                               ║ │
│  ║           ▼                                  ▼                               ║ │
│  ║   ┌──────────────────┐        ┌──────────────────────────────────────────┐  ║ │
│  ║   │   IoU预测         │        │       HyperNetwork MLPs ×4               │  ║ │
│  ║   │   (B, 4)          │        │   mask_tokens → MLP → 与upscaled特征     │  ║ │
│  ║   └──────────────────┘        │   进行矩阵乘法生成masks                   │  ║ │
│  ║                               └──────────────────────────────────────────┘  ║ │
│  ║                                              │                               ║ │
│  ║                                              ▼                               ║ │
│  ║                               ┌──────────────────────────────────────────┐  ║ │
│  ║                               │         low_res_masks                     │  ║ │
│  ║                               │         (B, 4, H/4, W/4)                  │  ║ │
│  ║                               └──────────────────────────────────────────┘  ║ │
│  ╚═════════════════════════════════════════════════════════════════════════════╝ │
│                                      │                                            │
│                                      ▼                                            │
│  ┌─────────────────────────────────────────────────────────────────────────────┐ │
│  │                    postprocess_masks (双线性插值上采样)                       │ │
│  │                    输出: (B, 1, H, W)                                         │ │
│  └─────────────────────────────────────────────────────────────────────────────┘ │
│                                      │                                            │
│                                      ▼                                            │
│  ┌─────────────────────────────────────────────────────────────────────────────┐ │
│  │                           最终分割掩码 (B, 1, H, W)                           │ │
│  └─────────────────────────────────────────────────────────────────────────────┘ │
└─────────────────────────────────────────────────────────────────────────────────┘
```

## 模块层级结构

```
Mobile_sam_adapter
│
├── image_encoder: TinyViT
│   │
│   ├── patch_embed: PatchEmbed
│   │   └── seq: Conv2d_BN → GELU → Conv2d_BN
│   │
│   ├── prompt_generator: PromptGenerator
│   │   ├── handcrafted_generator1-4: OverlapPatchEmbed
│   │   ├── embedding_generator1-4: nn.Linear
│   │   ├── lightweight_mlp{1-4}_{0-depth}: nn.Sequential
│   │   ├── shared_mlp{1-4}: nn.Linear
│   │   └── cross_attention: ConvPromptAdapter
│   │       ├── edge: MultiDirEdge
│   │       ├── msp: MultiScalePyramid
│   │       └── fuse3, to_cq, tune, up_gamma, up_beta
│   │
│   ├── layers[0]: ConvLayer
│   │   ├── blocks: ModuleList[MBConv × 2]
│   │   │   └── MBConv: conv1 → act1 → conv2 → act2 → conv3 → drop_path
│   │   └── downsample: PatchMerging
│   │
│   ├── layers[1-3]: BasicLayer
│   │   ├── blocks: ModuleList[TinyViTBlock × depth]
│   │   │   ├── attn: Attention (window attention)
│   │   │   ├── local_conv: Conv2d_BN (depthwise)
│   │   │   └── mlp: Mlp (LayerNorm → Linear → GELU → Linear)
│   │   └── downsample: PatchMerging (除stage4外)
│   │
│   ├── branch_mobile_v3: List[mobilenetv3_block]
│   │   └── MobileBottleneck: inverted bottleneck blocks
│   │
│   └── neck: Sequential
│       └── Conv2d → LayerNorm2d → Conv2d → LayerNorm2d
│
├── mask_decoder: MaskDecoder
│   │
│   ├── transformer: TwoWayTransformer
│   │   ├── layers: ModuleList[TwoWayAttentionBlock × 2]
│   │   │   ├── self_attn: Attention
│   │   │   ├── cross_attn_token_to_image: Attention
│   │   │   ├── mlp: MLPBlock
│   │   │   └── cross_attn_image_to_token: Attention
│   │   ├── final_attn_token_to_image: Attention
│   │   └── norm_final_attn: LayerNorm
│   │
│   ├── iou_token: Embedding(1, 256)
│   ├── mask_tokens: Embedding(4, 256)
│   │
│   ├── output_upscaling: Sequential
│   │   └── ConvT → LN → GELU → ConvT → GELU
│   │
│   ├── output_hypernetworks_mlps: ModuleList[MLP × 4]
│   └── iou_prediction_head: MLP
│
├── pe_layer: PositionEmbeddingRandom
│   └── positional_encoding_gaussian_matrix: Buffer
│
└── no_mask_embed: Embedding(1, 256)
```

## 数据维度变化流程

```
输入: (B, 3, 256, 256)  [以256×256输入为例]
        │
        ▼
┌─────────────────────────────────────────────────────┐
│ PatchEmbed                                          │
│ (B, 3, 256, 256) → (B, 64, 64, 64)                 │
└─────────────────────────────────────────────────────┘
        │
        ▼
┌─────────────────────────────────────────────────────┐
│ Stage 1 (ConvLayer)                                 │
│ (B, 64, 64, 64) → (B, 128, 32, 32)                 │
│ [MBConv×2 + PatchMerging]                          │
└─────────────────────────────────────────────────────┘
        │
        ▼
┌─────────────────────────────────────────────────────┐
│ Stage 2 (BasicLayer)                                │
│ (B, 32*32, 128) → (B, 16*16, 160)                  │
│ [TinyViTBlock×2 + PatchMerging]                    │
└─────────────────────────────────────────────────────┘
        │
        ▼
┌─────────────────────────────────────────────────────┐
│ Stage 3 (BasicLayer)                                │
│ (B, 16*16, 160) → (B, 16*16, 320)                  │
│ [TinyViTBlock×6 + PatchMerging(stride=1)]          │
└─────────────────────────────────────────────────────┘
        │
        ▼
┌─────────────────────────────────────────────────────┐
│ Stage 4 (BasicLayer)                                │
│ (B, 16*16, 320) → (B, 16*16, 320)                  │
│ [TinyViTBlock×2, 无下采样]                          │
└─────────────────────────────────────────────────────┘
        │
        ▼
┌─────────────────────────────────────────────────────┐
│ Neck                                                │
│ (B, 320, 16, 16) → (B, 256, 16, 16)               │
└─────────────────────────────────────────────────────┘
        │
        ▼
┌─────────────────────────────────────────────────────┐
│ TwoWayTransformer                                   │
│ image_embeddings: (B, 256, 16, 16)                 │
│ tokens: (B, 5, 256)  [1 iou + 4 mask tokens]       │
│ → queries: (B, 5, 256)                             │
│ → keys: (B, 256, 256)  [16*16=256]                 │
└─────────────────────────────────────────────────────┘
        │
        ▼
┌─────────────────────────────────────────────────────┐
│ Output Upscaling                                    │
│ (B, 256, 16, 16) → (B, 64, 32, 32) → (B, 32, 64, 64)│
└─────────────────────────────────────────────────────┘
        │
        ▼
┌─────────────────────────────────────────────────────┐
│ Mask Generation                                     │
│ hyper_in: (B, 4, 32)                               │
│ upscaled: (B, 32, 64*64) = (B, 32, 4096)           │
│ masks = hyper_in @ upscaled → (B, 4, 64, 64)       │
└─────────────────────────────────────────────────────┘
        │
        ▼
┌─────────────────────────────────────────────────────┐
│ Postprocess                                         │
│ (B, 1, 64, 64) → 双线性插值 → (B, 1, 256, 256)     │
└─────────────────────────────────────────────────────┘
        │
        ▼
输出: (B, 1, 256, 256)  [最终分割掩码]
```

## 关键参数配置

| 参数 | 值 | 说明 |
|------|-----|------|
| embed_dims | [64, 128, 160, 320] | 各Stage通道数 |
| depths | [2, 2, 6, 2] | 各Stage的Block数量 |
| num_heads | [2, 4, 5, 10] | 各Stage的注意力头数 |
| window_sizes | [7, 7, 14, 7] | 窗口注意力大小 |
| prompt_embed_dim | 256 | Prompt嵌入维度 |
| transformer_depth | 2 | TwoWayTransformer层数 |
| mlp_dim | 2048 | Transformer MLP隐藏层 |
| num_multimask_outputs | 3 | 多掩码输出数 |
